from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch
import os
import json 
import gc

# -----------------
# 1. ëª¨ë¸ ì„¤ì •
# -----------------
# EXAONE ê³µì‹ Instruct ëª¨ë¸ ê²½ë¡œ (ë¡œì»¬ ë‹¤ìš´ë¡œë“œ ìœ„ì¹˜)
# .envì— EXAONE_MODEL_PATH í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
MODEL_ID = os.getenv("EXAONE_MODEL_PATH", "/mnt/d/mequest/models/EXAONE-3.5-7.8B-Instruct")

# 4bit ì–‘ìí™” ì„¤ì • (RTX 5070 í™˜ê²½ ê¸°ì¤€)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.float16,
)

# -----------------
# 2. FastAPI ë° ëª¨ë¸ ë¡œë“œ
# -----------------
app = FastAPI(title="MeQuest EXAONE Feedback Generator", version="1.0.0")

try:
    print(f"Loading {MODEL_ID} (EXAONE-7.8B Instruct) ...")
    # tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_ID,
        trust_remote_code=True
    )

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        quantization_config=bnb_config,
        device_map="auto",
        low_cpu_mem_usage=True,
        trust_remote_code=True
    )
    print("âœ… EXAONE-7.8B Model Loaded Successfully on GPU!")
except Exception as e:
    print(f"âŒ Failed to load EXAONE model: {e}")
    # ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ë¯€ë¡œ ì„œë²„ ì‹œì‘ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.
    raise RuntimeError(f"Model Load Failed: {e}")

# -----------------
# 3. Pydantic ìš”ì²­/ì‘ë‹µ ëª¨ë¸ ì •ì˜
# -----------------
class GenerateRequest(BaseModel):
    """
    ì˜¤ë‹µ í”¼ë“œë°±ì„ ìœ„í•´ ë¬¸ì œ, ì‚¬ìš©ì ë‹µë³€, ì •ë‹µì„ ë°›ìŠµë‹ˆë‹¤.
    (Node.js llmService.jsì—ì„œ ì´ í•„ë“œëª…ì— ë§ì¶° ìš”ì²­ì„ ì „ì†¡í•´ì•¼ í•©ë‹ˆë‹¤.)
    """
    question: str                 # ì›ë˜ ë¬¸ì œ
    user_answer: str              # ì‚¬ìš©ì ë‹µë³€
    correct_answer: str           # ì •ë‹µ
    max_new_tokens: int = 512
    temperature: float = 0.5
    repetition_penalty: float = 1.2
    
class GenerateResponse(BaseModel):
    """ ì‘ë‹µ í¬ë§·ì„ GECKO/SOLARì™€ í†µì¼í•©ë‹ˆë‹¤. """
    model_id: str
    output: str 

# -----------------
# 4. API ì—”ë“œí¬ì¸íŠ¸
# -----------------

@app.get("/health")
async def health():
    """ ì„œë²„ ë° ëª¨ë¸ ë¡œë“œ ìƒíƒœ í™•ì¸ """
    return {"status": "ok", "model_loaded": model is not None}

@app.post("/feedback", response_model=GenerateResponse)
async def generate_feedback(request: GenerateRequest):
    """
    ì‚¬ìš©ìì˜ ì˜¤ë‹µì— ëŒ€í•´ EXAONE 7.8Bë¥¼ ì‚¬ìš©í•˜ì—¬ ê±´ì„¤ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤.
    """
    try:
        # EXAONE ì—­í• : ì˜¤ë‹µ í”¼ë“œë°± (ì¼ê´€ì„±ì„ ìœ„í•´ ê²°ì •ë¡ ì  ìƒ˜í”Œë§)
        system_prompt = (
            "ë„ˆëŠ” MeQuestì˜ ì˜¤ë‹µ í”¼ë“œë°± ì „ë¬¸ê°€ì´ë‹¤. "
            "ì‚¬ìš©ìê°€ ì œì¶œí•œ ë‹µë³€ì„ ë¶„ì„í•˜ê³ , ì™œ ì •ë‹µì´ ì˜¬ë°”ë¥¸ì§€ í•œêµ­ì–´ë¡œ 3~5 ë¬¸ì¥ìœ¼ë¡œ ì¹œì ˆí•˜ê³  ìƒì„¸í•˜ê²Œ ì„¤ëª…í•˜ë¼."
        )

        # EXAONE Instruct ëª¨ë¸ì˜ í”„ë¡¬í”„íŠ¸ í¬ë§· (Mistral/Llama Instruct í¬ë§· ì‚¬ìš© ê°€ì •)
        prompt_template = f"""<s>[INST] {system_prompt}

ì§ˆë¬¸: {request.question}
ì‚¬ìš©ì ë‹µë³€: {request.user_answer}
ì •ë‹µ: {request.correct_answer} [/INST]"""
        
        # 1. í† í°í™” ë° GPU ì´ë™
        inputs = tokenizer(prompt_template, return_tensors="pt")
        # token_type_ids ì œê±° ë¡œì§ì€ EXAONE ëª¨ë¸ì—ì„œ ë¶ˆí•„ìš”í•˜ë‹¤ê³  ê°€ì •í•˜ê³  ì œê±°
        inputs = inputs.to(model.device)   
        
        # 2. í…ìŠ¤íŠ¸ ìƒì„± (ê²°ì •ë¡ ì  ìƒ˜í”Œë§)
        outputs = model.generate(
            **inputs,
            max_new_tokens=request.max_new_tokens,
            temperature=request.temperature,
            do_sample=(request.temperature > 0), # â† ì¡°ê±´ë¶€ ìƒ˜í”Œë§
            repetition_penalty=request.repetition_penalty,
            pad_token_id=tokenizer.eos_token_id
        )

        # 3. ê²°ê³¼ ë””ì½”ë”© ë° í›„ì²˜ë¦¬
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì œê±°
        if '[/INST]' in generated_text:
            feedback = generated_text.split('[/INST]', 1)[-1].strip()
        else:
            feedback = generated_text.strip()
            
        # VRAM ì •ë¦¬ (ì•ˆì •ì„± í™•ë³´)
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            
        return GenerateResponse(model_id=MODEL_ID, output=feedback)

    except Exception as e:
        print(f"âŒ Feedback Generation failed: {e}")
        raise HTTPException(status_code=500, detail=f"Feedback Generation failed: {e}")

if __name__ == "__main__":
    import uvicorn
    # ğŸ’¡ 8003 í¬íŠ¸ ì‚¬ìš©
    uvicorn.run(app, host="0.0.0.0", port=8003) 

# VRAM ì •ë¦¬
import gc
gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()

