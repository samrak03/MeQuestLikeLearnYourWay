from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch

# -----------------
# 1. ëª¨ë¸ ì„¤ì •
# Instruct ë²„ì „ ì‚¬ìš©ì„ ê°•ë ¥íˆ ê¶Œì¥í•©ë‹ˆë‹¤. (ì§€ì‹œ ìˆ˜í–‰ ëŠ¥ë ¥ í–¥ìƒ)
# ë§Œì•½ ë¡œì»¬ ê²½ë¡œë¥¼ ì‚¬ìš©í•œë‹¤ë©´: MODEL_ID = "/mnt/d/mequest/models/SOLAR-10.7B-Instruct-v1.0"

MODEL_ID = "/mnt/d/mequest/models/SOLAR-10.7B-Instruct-v1.0"

# 4bit ì–‘ìí™” ì„¤ì • (RTX 5070 í™˜ê²½ ê¸°ì¤€)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.float16,
)

# -----------------
# 2. FastAPI ë° ëª¨ë¸ ë¡œë“œ
# -----------------
app = FastAPI(title="MeQuest SOLAR-10.7B Summarizer", version="1.0.1")

try:
    print(f"Loading {MODEL_ID} ...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        quantization_config=bnb_config,
        device_map="auto",
        low_cpu_mem_usage=True
    )
    print("âœ… SOLAR-10.7B Model Loaded Successfully")
except Exception as e:
    print(f"âŒ Failed to load SOLAR model: {e}")
    # ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì„œë²„ë¥¼ ê°•ì œ ì¢…ë£Œí•˜ì—¬ ë¬´ê±°ìš´ ëª¨ë¸ì´ ë©”ëª¨ë¦¬ë§Œ ì°¨ì§€í•˜ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤.
    raise RuntimeError(f"Model Load Failed: {e}")

# -----------------
# 3. Pydantic ìš”ì²­/ì‘ë‹µ ëª¨ë¸ ì •ì˜
# -----------------
class GenerateRequest(BaseModel):
    """
    Node.js ë°±ì—”ë“œì™€ì˜ ì¼ê´€ì„±ì„ ìœ„í•´ 'text' í•„ë“œë¡œ í†µì¼í•©ë‹ˆë‹¤.
    """
    document: str                     # ìš”ì•½í•  ë¬¸ì„œ/í…ìŠ¤íŠ¸
    max_new_tokens: int = 512
    temperature: float = 0.5
    repetition_penalty: float = 1.2

# ì‘ë‹µì€ ë‹¨ìˆœ í…ìŠ¤íŠ¸ ë°˜í™˜
class GenerateResponse(BaseModel):
    model_id: str
    summary: str
    
# -----------------
# 4. API ì—”ë“œí¬ì¸íŠ¸
# -----------------

@app.get("/health")
async def health():
    """ëª¨ë¸ ë° ì„œë²„ ìƒíƒœ í™•ì¸"""
    return {"status": "ok", "model_loaded": model is not None}

@app.post("/summarize", response_model=GenerateResponse)
async def summarize(request: GenerateRequest):
    """
    ë¬¸ì„œë¥¼ ë°›ì•„ SOLAR-10.7Bë¥¼ ì‚¬ìš©í•˜ì—¬ ìš”ì•½í•©ë‹ˆë‹¤.
    """
    try:
        # SOLAR-10.7Bì˜ ì—­í• : ìš”ì•½ ë° ì‹œê°í™” ì¤€ë¹„
        system_prompt = (
            "ë„ˆëŠ” í•œêµ­ì–´ êµìœ¡ í”Œë«í¼ì„ ìœ„í•œ ì „ë¬¸ ìš”ì•½ì…ë‹ˆë‹¤."
            "ëª¨ë“  ì‘ë‹µì€ **ë°˜ë“œì‹œ í•œêµ­ì–´(Korean)ë¡œë§Œ** ì‘ì„±í•´ì•¼ í•˜ë©°, ë‹¤ë¥¸ ì–¸ì–´ë‚˜ ë¶ˆí•„ìš”í•œ ë§ˆì»¤([SOLUTION], [RESULT] ë“±)ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”. "
            "ë¬¸ì„œë¥¼ í•µì‹¬ë§Œ ìš”ì•½í•˜ì„¸ìš”."
        )

        # ğŸ’¡ SOLAR Instruct ëª¨ë¸ì˜ ëŒ€í™” í…œí”Œë¦¿ (Mistral í¬ë§· ì‚¬ìš©)
        prompt_template = f"<s>[INST] {system_prompt}\n\nDocument: {request.document} [/INST]"
        
        # 1. í† í°í™”
        inputs = tokenizer(prompt_template, return_tensors="pt")
        inputs.pop("token_type_ids", None)   # ë¶ˆí•„ìš”í•œ key ì œê±°
        inputs = inputs.to(model.device)
        
        # 2. í…ìŠ¤íŠ¸ ìƒì„±
        outputs = model.generate(
            **inputs,
            max_new_tokens=request.max_new_tokens,
            temperature=request.temperature,
            do_sample=False,
            # repetition_penalty=1.2,
            repetition_penalty=request.repetition_penalty,
            pad_token_id=tokenizer.eos_token_id
        )

        # 3. ê²°ê³¼ ë””ì½”ë”© ë° í›„ì²˜ë¦¬ (í”„ë¡¬í”„íŠ¸ ì œê±°)
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()
        
        # ğŸ’¡ ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ ëª¨ë¸ì˜ ë‹µë³€ë§Œ ì¶”ì¶œ (ì…ë ¥ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì œê±°)
        # generated_textëŠ” ë³´í†µ "<s>[INST]...[/INST] ë‹µë³€" í˜•íƒœë¡œ ë‚˜ì˜¤ë¯€ë¡œ, [/INST] ì´í›„ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        if '[/INST]' in generated_text:
            summary = generated_text.split('[/INST]', 1)[-1].strip()
        else:
            summary = generated_text.strip() # í…œí”Œë¦¿ì´ ì—†ì„ ê²½ìš° ì „ì²´ ë°˜í™˜
        

        # ğŸ’¡ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (VRAM ì•ˆì •ì„± í–¥ìƒ)
        if torch.cuda.is_available():
            torch.cuda.empty_cache()


        return GenerateResponse(model_id=MODEL_ID, summary=summary)

    except Exception as e:
        print(f"âŒ Summarization failed: {e}")
        # ì¶”ë¡  ì¤‘ OOM ì˜¤ë¥˜ ë“± ë°œìƒ ì‹œ 500 ì—ëŸ¬ ë°˜í™˜
        raise HTTPException(status_code=500, detail=f"Summarization failed: {e}")

if __name__ == "__main__":
    import uvicorn
    # ğŸ’¡ í¬íŠ¸ 8002 ì‚¬ìš©
    uvicorn.run(app, host="0.0.0.0", port=8002)
